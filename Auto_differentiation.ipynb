{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba349c1e-5205-4bf8-ba04-3bc60c04ac75",
   "metadata": {},
   "source": [
    "# **자동 차별화**\n",
    "신경망 훈련시 자주 사용되는 알고리즘은 **역전파**이다.    \n",
    "역전파에서 매개변수는 주어진 매개변수에 대한 손실 함수의 **기울기**에 따라 조정된다.   \n",
    "**파이토치는 그라이디언트 계산을 위해 `torch.autograd`를 지원한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d20891e-3236-4af4-8a29-7f2b55338aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)  # 네트워크의 가중치와 편향 최적화를 위해 `requires_grade_(True)` 속성을 설정한다.\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48783b2-4644-44cc-b47d-08e14d729fb9",
   "metadata": {},
   "source": [
    "### **그라디언트 계산**\n",
    "신경망에서 매개변수의 가중치를 최적화 하려면 매개변수와 관련해 손실함수의 도함수를 계산해야한다.   \n",
    "이러한 미분을 계산하기 위해 `loss.backward()` 호출 후 `w.grad` 및 `b.grad`로 값을 검색한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbcfa170-f900-4731-8b8d-06d0b1a8e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0543, 0.2549, 0.3251],\n",
      "        [0.0543, 0.2549, 0.3251],\n",
      "        [0.0543, 0.2549, 0.3251],\n",
      "        [0.0543, 0.2549, 0.3251],\n",
      "        [0.0543, 0.2549, 0.3251]])\n",
      "tensor([0.0543, 0.2549, 0.3251])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe46a7-f5e8-4e83-9d9c-f943713a406e",
   "metadata": {},
   "source": [
    "### **그라디언트 추적 비활성화**\n",
    "기본적으로는 모든 텐서가 `requires_grad=True`로 계산 기록을 추적하지만 그럴 필요가 없을때 `torch.no_grad()`나 `detach()`로 추적을 중지 할 수 잇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b483e25f-2a1a-4416-8073-b3aca6702d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05b2e4-9cf6-4619-80b1-0fbee8302363",
   "metadata": {},
   "source": [
    "### **그래디언트 추적 비활성화 이유**\n",
    "* __고정 된 매개변수__로 신경망의 일부 매개 변수를 표시합니다.\n",
    "* 계산 속도를 올리기 위해서\n",
    "\n",
    "### **계산 그래프에 대한 추가 정보**\n",
    "개념적으로 오토그래드는 객체로 도니 DAG에 데이터 실행의 모든 작업을 유지한다.   \n",
    "이 DAG에서 잎은 입력텐서, 뿌리는 출력텐서이다.   \n",
    "**정방향 패스**에서 qutograd는 `요청된 작업 실행결과 텐서를 계산`, `DAG에서 작업의 기울기 기능을 유지합니다.`   \n",
    "**역방향 패스** 는 DAG 루트에서 호출될 때 시작되며 기울기 계산(`.grad_fn`), 각 텐서의 `.grad` 속성에 누적, 체인 규칙으로 리프 텐서까지 전파등을 수행한다.  \n",
    "\n",
    "**DAG***는 파이토치에서 동적이다. 또한 `.backword()` 호출 후 autograd는 새 그래프를 채우기 시작한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a02690b-ade0-46f1-a453-307e54b47168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57792b-e35b-43c5-b03b-e4b613414b04",
   "metadata": {},
   "source": [
    "`backward`를 동일한 인수로 사용해도 그래디언트 값이 다르다.   \n",
    "이것은 `backward` 전파이 파이토치가 __그래디언트 축적__하기 때문이다.__   \n",
    "즉, 계산된 그래디언트 값이 게산 `grad` 그래프의 모든 리프 노드에 추가된다...   \n",
    "적절한 그래디언트 계산을 위해서는 grad 이전에 속성을 0으로 해야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53cb3f-ebf6-4e96-bf49-4cf8772139d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
