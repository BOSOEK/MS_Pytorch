{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254e6aa0-899b-48c1-837a-34b45c42c5cc",
   "metadata": {},
   "source": [
    "# Pre-Trained tokenizer 사용하기   \n",
    "BPE Tokenizer는 구글의 Sentencepiece를 많이 사용한다(Sentencepiece 사용법 : https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0262aef-1381-4cdc-b5e8-bb0b9d37ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "# 이미 학습된 Tokenizer와 vocabulary 가져오기\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # bert-base-uncased라는 모델을 사용하기 위해 Tokenizer을 일치시키기 위해 미리 학습된 Tokenizer가져옴\n",
    "\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513d7783-c906-4eb2-a875-4bc2def34ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cute', '.', 'he', 'likes', 'playing']\n",
      "105879\n",
      "['my', 'dog', 'is', 'cut', '##e', '.', 'he', 'likes', 'playing']\n",
      "['나는', 'ᄎ', '##ᅢᆨ', '##상', '위에', 'ᄉ', '##ᅡ', '##과', '##를', 'ᄆ', '##ᅥ', '##ᆨ', '##었다', '.', '알', '##고', 'ᄇ', '##ᅩ', '##니', '그', 'ᄉ', '##ᅡ', '##과', '##는', 'jason', '것이', '##었다', '.', '그', '##래', '##서', 'jason', '##에게', 'ᄉ', '##ᅡ', '##과', '##를', '했다']\n"
     ]
    }
   ],
   "source": [
    "# 토크니저가 문장을 Tokenization하는 방법\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence = 'My dog is cute. He likes playing'\n",
    "print(tokenizer.tokenize(sentence))                                       # tokenization 결과 보기\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "print(len(tokenizer.vocab))\n",
    "print(tokenizer.tokenize(sentence))                                       # \n",
    "\n",
    "sentence = '나는 책상 위에 사과를 먹었다. 알고 보니 그 사과는 Jason 것이었다. 그래서 Jason에게 사과를 했다'\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f9d837b-6c09-4df2-b7b0-92ba6dc4b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 및 Pre-Trained Embedding Vector를 이용한 Vocabulary 생성\n",
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "# Data Setting\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                 fix_length = 500,       # Sentence 길이 제한\n",
    "                 tokenize=str.split,     # Tokenize를 설정하는 옵션 띄어쓰기 기반으로 설정\n",
    "                 pad_first = True,       # fix_length 보다 짧은 경우 패딩을 앞에하라\n",
    "                 pad_token='[PAD]',      # 패딩에 대한 특수 토큰 설정\n",
    "                 unk_token='[UNK]')      # 사전에 없는 토큰의 경우 특수 토큰\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.float)  # 가져올 데이터에 대한 타입 설정\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c8fae3-6633-427f-a54c-c251f351f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length: 25000\n",
      "Test Data Length: 25000\n",
      "{'text': <torchtext.legacy.data.field.Field object at 0x00000251431D3E50>, 'label': <torchtext.legacy.data.field.LabelField object at 0x00000251431D3EB0>}\n",
      "--- Data Sample ---\n",
      "Input : \n",
      "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others. \\n\n",
      "Label : \n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# 불러온 데이터 확인\n",
    "\n",
    "# 데이터 길이\n",
    "print(f'Train Data Length: {len(train_data.examples)}')\n",
    "print(f'Test Data Length: {len(test_data.examples)}')\n",
    "\n",
    "# 데이터 필드\n",
    "print(train_data.fields)\n",
    "\n",
    "# 데이터 샘플\n",
    "print('--- Data Sample ---')\n",
    "print('Input : ')\n",
    "print(' '.join(vars(train_data.examples[1])['text']), '\\\\n')\n",
    "print('Label : ')\n",
    "print(vars(train_data.examples[1])['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab6051e-c113-4aa6-a437-25eceda4e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 클리닝 작업하기\n",
    "import re\n",
    "\n",
    "def PreProcessingText(input_sentence):\n",
    "    input_sentence = input_sentence.lower()\n",
    "    input_sentence = re.sub('<[^>]*>', repl=' ', string = input_sentence)  # <br /> 처리\n",
    "    input_sentence = re.sub('[!\"#$%&\\\\()*+, -./:;<=>?@[\\\\\\\\]^_`{|}~\"]', repl= ' ', string = input_sentence)  # 특수 문자 처리\n",
    "    input_sentence = re.sub('\\\\s+', repl=' ', string = input_sentence)  # 연속된 띄어쓰기 처리\n",
    "    if input_sentence:\n",
    "        return input_sentence\n",
    "for example in train_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n",
    "for example in test_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3cc052e-38d3-45ad-abbe-681344723481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [12:30, 1.15MB/s]                                                                    \n",
      "100%|███████████████████████████████████████████████████████████████████████▉| 399999/400000 [01:03<00:00, 6291.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# 주어진 데이터로 토큰 vocabulary를 만들기\n",
    "TEXT.build_vocab(train_data,\n",
    "                min_freq = 2,                # Vocab에 해당하는 토큰에 최소한으로 등장하는 횟수에 제한을 둔다\n",
    "                max_size = None,             # 토큰의 최소 등장 회수로 vocab 의 사이즈를 조절하는 것 외에 전체 Vocab 자체에 제한을 둔다\n",
    "                vectors = 'glove.6B.300d')   # 사전 학습 Vector를 가져와 Vocab에 세팅하는 옵션. 원하는 임베딩을 string 형태로 설정\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff53d12f-308e-40e9-a651-b86fd6b5f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 만들기\n",
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(0), split_ratio=0.8)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data), batch_size=30, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e5afc-d6ff-40b1-b543-2d63e1b0ed7e",
   "metadata": {},
   "source": [
    "## Models\n",
    "NLP 모델로는 RNN, LSTM, Bi-RNNs, GRUs 등이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621da7f1-f42c-4a4b-8e36-6c7db6e4c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SentenceClassification(nn.Module):\n",
    "    def __init__(self, **model_config):\n",
    "        super(sentenceClassification, self).__init__()\n",
    "        \n",
    "        if model_config['emb_type'] == 'glove' or 'fasttext':\n",
    "            self.emb = nn.Embedding(model_config['vocab_size'],   # 임베딩 설정, num_embedding  : Vocab Size\n",
    "                                   model_config['emb_dim'],       # embedding_dim : 원하는 임베딩 Dimension 설정\n",
    "                                   _weight = TEXT.vocab.vectors)  # \n",
    "        else:\n",
    "            self.emb = nn.Embedding(model_config['vocab_size'],\n",
    "                                   model_config['emb_dim'])\n",
    "        self.bidirectional = model_config['bidirectional']\n",
    "        self.num_direction = 2 if model_config['bidirectional'] else 1\n",
    "        self.model_type = model_config['model_type']\n",
    "        \n",
    "        self.RNN = nn.RNN(input_size = model_config['emb_dim'],         # 입력받을 데이터 크기\n",
    "                         hidden_size = model_config['hidden_dim'],      # Hidden Layer의 Dimension 설정\n",
    "                         dropout = model_config['dropout'],             # 드롭 아웃의 확률 설정\n",
    "                         bidirectional = model_config['bidirectional'], # 양방향 모델 사용시 설정\n",
    "                         batch_first = model_config['batch_first'])     # Data의 제일 처음 Axis에 배치 사이즈가 오도록 설정\n",
    "        self.LSTM = nn.LSTM(input_size = model_config['emb_dim'],\n",
    "                           hidden_size = model_config['hidden_dim'],\n",
    "                           dropout = model_config['dropout'],\n",
    "                           bidirectinal = model_config['bidirectional'],\n",
    "                           batch_first = model_config['batch_first'])\n",
    "        self.GRU = nn.GRU(input_size = model_config['emb_dim'],\n",
    "                         hidden_size = model_config['hidden_dim'],\n",
    "                         dropout = model_config['dropout'],\n",
    "                         bidirectinal = model_config['bidirectional'],\n",
    "                         batch_first = model_config['batch_first'])\n",
    "        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction, model_config['output_dim']) \n",
    "        self.drop = nn.Dropout(model_config['dropout'])\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        if self.model_type == 'RNN':\n",
    "            output, hidden = self.RNN(emb)\n",
    "        elif self.model_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.LSTM(emb)\n",
    "        elif self.model_type == 'GRU':\n",
    "            output, hidden = self.GRU(emb)\n",
    "        else:\n",
    "            raise NameError('Select model_type in [RNN, LSTM, GRU]')\n",
    "        last_output = output[:,-1,:]\n",
    "        return self.fc(self.drop(last_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87678b2f-9653-45cf-8f8d-db17fdabbd70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b27552ace838>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_config.update(dict(batch_first = True,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         \u001b[0mmodel_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'RNN'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[0mbidirectional\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                         \u001b[0mhidden_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                         \u001b[0moutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_config' is not defined"
     ]
    }
   ],
   "source": [
    "model_config.update(dict(batch_first = True,\n",
    "                        model_type = 'RNN',\n",
    "                        bidirectional = True,\n",
    "                        hidden_dim = 128,\n",
    "                        output_dim = 1,\n",
    "                        dropout = 0))\n",
    "model = SentenceClassification(**model_config)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "predictions = model.forward(sample_for_check.text).squeeze()\n",
    "loss = loss_fn(predictions, sample_for_check.label)\n",
    "acc = binary_accuracy(predictions, sample_for_chec.label)\n",
    "\n",
    "print(predictions)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67ea75-9960-4d4d-88d5-861ac8a50379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
