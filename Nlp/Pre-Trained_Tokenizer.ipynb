{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254e6aa0-899b-48c1-837a-34b45c42c5cc",
   "metadata": {},
   "source": [
    "# Pre-Trained tokenizer 사용하기   \n",
    "BPE Tokenizer는 구글의 Sentencepiece를 많이 사용한다(Sentencepiece 사용법 : https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0262aef-1381-4cdc-b5e8-bb0b9d37ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "# 이미 학습된 Tokenizer와 vocabulary 가져오기\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # bert-base-uncased라는 모델을 사용하기 위해 Tokenizer을 일치시키기 위해 미리 학습된 Tokenizer가져옴\n",
    "\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513d7783-c906-4eb2-a875-4bc2def34ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cute', '.', 'he', 'likes', 'playing']\n",
      "105879\n",
      "['my', 'dog', 'is', 'cut', '##e', '.', 'he', 'likes', 'playing']\n",
      "['나는', 'ᄎ', '##ᅢᆨ', '##상', '위에', 'ᄉ', '##ᅡ', '##과', '##를', 'ᄆ', '##ᅥ', '##ᆨ', '##었다', '.', '알', '##고', 'ᄇ', '##ᅩ', '##니', '그', 'ᄉ', '##ᅡ', '##과', '##는', 'jason', '것이', '##었다', '.', '그', '##래', '##서', 'jason', '##에게', 'ᄉ', '##ᅡ', '##과', '##를', '했다']\n"
     ]
    }
   ],
   "source": [
    "# 토크니저가 문장을 Tokenization하는 방법\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence = 'My dog is cute. He likes playing'\n",
    "print(tokenizer.tokenize(sentence))                                       # tokenization 결과 보기\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "print(len(tokenizer.vocab))\n",
    "print(tokenizer.tokenize(sentence))                                       # \n",
    "\n",
    "sentence = '나는 책상 위에 사과를 먹었다. 알고 보니 그 사과는 Jason 것이었다. 그래서 Jason에게 사과를 했다'\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d837b-6c09-4df2-b7b0-92ba6dc4b348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
