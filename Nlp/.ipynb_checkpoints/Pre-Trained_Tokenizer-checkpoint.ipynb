{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254e6aa0-899b-48c1-837a-34b45c42c5cc",
   "metadata": {},
   "source": [
    "# Pre-Trained tokenizer 사용하기   \n",
    "BPE Tokenizer는 구글의 Sentencepiece를 많이 사용한다(Sentencepiece 사용법 : https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0262aef-1381-4cdc-b5e8-bb0b9d37ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "# 이미 학습된 Tokenizer와 vocabulary 가져오기\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # bert-base-uncased라는 모델을 사용하기 위해 Tokenizer을 일치시키기 위해 미리 학습된 Tokenizer가져옴\n",
    "\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513d7783-c906-4eb2-a875-4bc2def34ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cute', '.', 'he', 'likes', 'playing']\n",
      "105879\n",
      "['my', 'dog', 'is', 'cut', '##e', '.', 'he', 'likes', 'playing']\n",
      "['나는', 'ᄎ', '##ᅢᆨ', '##상', '위에', 'ᄉ', '##ᅡ', '##과', '##를', 'ᄆ', '##ᅥ', '##ᆨ', '##었다', '.', '알', '##고', 'ᄇ', '##ᅩ', '##니', '그', 'ᄉ', '##ᅡ', '##과', '##는', 'jason', '것이', '##었다', '.', '그', '##래', '##서', 'jason', '##에게', 'ᄉ', '##ᅡ', '##과', '##를', '했다']\n"
     ]
    }
   ],
   "source": [
    "# 토크니저가 문장을 Tokenization하는 방법\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence = 'My dog is cute. He likes playing'\n",
    "print(tokenizer.tokenize(sentence))                                       # tokenization 결과 보기\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "print(len(tokenizer.vocab))\n",
    "print(tokenizer.tokenize(sentence))                                       # \n",
    "\n",
    "sentence = '나는 책상 위에 사과를 먹었다. 알고 보니 그 사과는 Jason 것이었다. 그래서 Jason에게 사과를 했다'\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9d837b-6c09-4df2-b7b0-92ba6dc4b348",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-23fb3ce1607b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMDB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'splits'"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 및 Pre-Trained Embedding Vector를 이용한 Vocabulary 생성\n",
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext import datasets\n",
    "\n",
    "# Data Setting\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                 fix_length = 500,\n",
    "                 tokenize=str.split,\n",
    "                 pad_first = True,\n",
    "                 pad_token='[PAD]',\n",
    "                 unk_token='[UNK]')\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8fae3-6633-427f-a54c-c251f351f260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
